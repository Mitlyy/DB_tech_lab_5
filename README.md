## Цель работы

Разработать и запустить Spark-приложение на PySpark, выполняющее: (1) загрузку и первичный обзор больших данных, (2) предобработку и формирование признаков, (3) обучение модели k-means с оценкой качества, (4) сохранение артефактов и предсказаний. Требования и формат результата соответствуют методичке по ЛР-5 («Модель кластеризации на PySpark»).

## Исходные данные
Использована публичная база Open Food Facts (OFF). Для удобства и стабильности применялся «CSV»-экспорт.
world.openfoodfacts.org
static.openfoodfacts.org

## Среда и подготовка
**Python 3.12, PySpark 3.5.x**
Проверка установки: скрипт wordcount.py в локальном режиме Spark.

## Структура проекта (ключевые файлы)
```
DB_tech_lab_5/
├── scripts/
│   ├── download_off.sh         # загрузка TSV/JSONL с проверкой целостности gzip
│   ├── peek_off.py             # быстрый обзор: схема, первые строки, describe(), null-count
│   ├── preprocess_off.py       # предобработка + сборка feature-вектора + parquet (full/sample)
│   ├── train_kmeans.py         # перебор k, silhouette, сохранение лучшей модели и витрины
│   └── predict_off.py          # инференс на полном parquet с использованием сохранённой модели
├── explore_data.py             # вспомогательное исследование (локальные проверки)
├── wordcount.py                # sanity-check установки Spark
└── README.md

```
---

## Ход выполнения
### Загрузка и первичный обзор
Загрузка: `scripts/download_off.sh` csv — скачивание en.openfoodfacts.org.products.csv.gz с повторными попытками и проверкой gzip.

Обзор: `scripts/peek_off.py` — печать первых колонок, printSchema(), head(20), describe() по нутриентам, счётчики пропусков.

---

### Предобработка и формирование признаков
Скрипт `scripts/preprocess_off.py`:
Нормализация числовых полей (замена ,→.), явный cast в Double.
Мягкая фильтрация по разумным диапазонам (г/100г в [0;100], натрий ≤ 40, энергия ≤ 10 000).
VectorAssembler + StandardScaler ⇒ колонка features.
Сохранение: data/clean/products_features.parquet (полный) и data/clean/products_sample.parquet (сэмпл для быстрых прогонов).

---

### Обучение k-means и выбор k
Скрипт `scripts/train_kmeans.py`:
Читает `products_sample.parquet` (или полный parquet при необходимости).
Автоматически выбирает доступные нутриентные столбцы (с поддержкой синонимов имён: energy-kcal_100g / energy-kj_100g и т. д.).
Перебор k из набора (например, 3–10), метрика silhouette.
Сохранение лучшей модели в `artifacts/kmeans/model_k<k>` и генерация витрины: размеры кластеров, средние значения по «сырым» фичам, примеры продуктов (name/brand/country).

---

### Результаты
Подготовлен воспроизводимый конвейер: загрузка → обзор → чистка/признаки → кластеризация → инференс/витрины.
Получена модель k-means с отобранным по silhouette числом кластеров k
Сохранены артефакты: модель, parquet-предсказания, профили кластеров.
